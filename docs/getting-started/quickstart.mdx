---
title: "Getting Started"
sidebar_position: 1
description: Get started with Karpenter in minutes
---

import ProviderContent from '@site/src/components/ProviderContent/ProviderContent';
import ProviderVariable from '@site/src/components/ProviderVariable/ProviderVariable';

<ProviderContent providers={["azure"]}>
This quickstart will help you deploy Karpenter for Azure on your AKS cluster. Karpenter automatically provisions right-sized nodes based on your workload requirements, improving efficiency and reducing costs.

## Choose Your Deployment Mode

Karpenter for Azure supports two deployment modes:

- **Node Auto Provisioning (NAP)** - Managed by AKS (Recommended)
- **Self-hosted** - You manage the deployment

## Node Auto Provisioning (NAP)

NAP is the recommended way to use Karpenter with AKS. Microsoft manages the entire Karpenter lifecycle as a managed addon.

### Prerequisites

- Azure subscription
- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli) installed
- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) installed

### Create AKS Cluster with NAP

Set up your environment:

```bash
# Set environment variables
export CLUSTER_NAME=karpenter-nap-cluster
export RESOURCE_GROUP=karpenter-demo
export LOCATION=westus2

# Login to Azure
az login
```

Create the resource group:

```bash
az group create --name $RESOURCE_GROUP --location $LOCATION
```

Create AKS cluster with NAP enabled:

```bash
az aks create \
  --resource-group $RESOURCE_GROUP \
  --name $CLUSTER_NAME \
  --location $LOCATION \
  --enable-node-autoprovision \
  --node-count 1 \
  --generate-ssh-keys
```

Get cluster credentials:

```bash
az aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME
```

### Verify Installation

Check that NAP is enabled:

```bash
kubectl get nodes -o wide
kubectl get nodepool
```

## Self-hosted Mode

For advanced users who need more control or want to run Karpenter in non-AKS environments.

### Prerequisites

- Azure subscription
- Azure CLI installed
- kubectl installed
- Helm installed
- An existing AKS cluster or Azure Kubernetes cluster

### Install Karpenter

Add the Karpenter Helm repository:

```bash
helm repo add karpenter-azure https://azure.github.io/karpenter-provider-azure
helm repo update
```

Install Karpenter:

```bash
helm install karpenter karpenter-azure/karpenter \
  --namespace karpenter \
  --create-namespace \
  --set controller.clusterName=$CLUSTER_NAME
```

### Create NodeClass and NodePool

Create an AKSNodeClass:

```yaml
apiVersion: karpenter.azure.com/v1beta1
kind: AKSNodeClass
metadata:
  name: default
spec:
  imageFamily: Ubuntu2204
  osDiskSizeGB: 128
  tags:
    managed-by: karpenter
```

Create a NodePool:

```yaml
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      nodeClassRef:
        apiVersion: karpenter.azure.com/v1beta1
        kind: AKSNodeClass
        name: default
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: karpenter.azure.com/sku-family
          operator: In
          values: ["D", "E"]
  disruption:
    consolidationPolicy: WhenUnderutilized
```
</ProviderContent>

<ProviderContent providers={["aws"]}>
This guide shows how to get started with Karpenter by creating an EKS cluster and installing Karpenter.

Karpenter automatically provisions new nodes in response to unschedulable pods by observing events within the Kubernetes cluster and sending commands to the underlying cloud provider.

## Create a cluster and add Karpenter

This guide uses `eksctl` to create the cluster. It should take less than 1 hour to complete and cost less than $0.25. Follow the clean-up instructions to reduce any charges.

### 1. Install utilities

Karpenter is installed in clusters with a Helm chart. Karpenter requires cloud provider permissions to provision nodes. For AWS, IAM Roles for Service Accounts (IRSA) should be used.

Install these tools before proceeding:

1. [AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html)
2. [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/) - the Kubernetes CLI
3. [eksctl](https://eksctl.io/installation) (>= v0.202.0) - the CLI for AWS EKS
4. [helm](https://helm.sh/docs/intro/install/) - the package manager for Kubernetes

[Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html) with a user that has sufficient privileges to create an EKS cluster. Verify that the CLI can authenticate properly by running `aws sts get-caller-identity`.

### 2. Set environment variables

After setting up the tools, set the Karpenter and Kubernetes version:

```bash
export KARPENTER_NAMESPACE="karpenter"
export KARPENTER_VERSION="1.0.8"
export K8S_VERSION="1.31"
export CLUSTER_NAME="${USER}-karpenter-demo"
export AWS_DEFAULT_REGION="us-west-2"
export AWS_ACCOUNT_ID="$(aws sts get-caller-identity --query Account --output text)"
export TEMPOUT="$(mktemp)"
```

### 3. Create a Cluster

Create a cluster with `eksctl`. This example configuration file specifies a basic cluster with one initial node and sets up an IAM OIDC provider for the cluster to enable IAM roles for pods:

```bash
curl -fsSL https://raw.githubusercontent.com/aws/karpenter-provider-aws/v"${KARPENTER_VERSION}"/website/content/en/docs/getting-started/getting-started-with-karpenter/cloudformation.yaml > "${TEMPOUT}" \
&& aws cloudformation deploy \
  --stack-name "Karpenter-${CLUSTER_NAME}" \
  --template-body "file://${TEMPOUT}" \
  --capabilities CAPABILITY_IAM \
  --parameter-overrides "ClusterName=${CLUSTER_NAME}"
```

Create the cluster:

```bash
eksctl create cluster -f - <<EOF
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ${CLUSTER_NAME}
  region: ${AWS_DEFAULT_REGION}
  version: "${K8S_VERSION}"
  tags:
    karpenter.sh/discovery: ${CLUSTER_NAME}

iam:
  withOIDC: true
  serviceAccounts:
  - metadata:
      name: karpenter
      namespace: "${KARPENTER_NAMESPACE}"
    roleName: ${CLUSTER_NAME}-karpenter
    attachPolicyARNs:
    - arn:aws:iam::${AWS_ACCOUNT_ID}:policy/KarpenterControllerPolicy-${CLUSTER_NAME}
    roleOnly: true

iamIdentityMappings:
- arn: "arn:aws:iam::${AWS_ACCOUNT_ID}:role/KarpenterNodeInstanceProfile-${CLUSTER_NAME}"
  username: system:node:{{EC2PrivateDNSName}}
  groups:
  - system:bootstrappers
  - system:nodes

managedNodeGroups:
- instanceType: m5.large
  amiFamily: AmazonLinux2
  name: ${CLUSTER_NAME}-ng
  desiredCapacity: 2
  minSize: 1
  maxSize: 10

addons:
- name: eks-pod-identity-agent
EOF
```

Tag the subnets and security groups:

```bash
for NODEGROUP in $(aws eks list-nodegroups --cluster-name "${CLUSTER_NAME}" --query 'nodegroups[0]' --output text); do
    aws ec2 create-tags \
        --tags "Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}" \
        --resources $(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" \
        --nodegroup-name "${NODEGROUP}" --query 'nodegroup.subnets' --output text )
done

NODEGROUP=$(aws eks list-nodegroups --cluster-name "${CLUSTER_NAME}" --query 'nodegroups[0]' --output text)

LAUNCH_TEMPLATE=$(aws eks describe-nodegroup --cluster-name "${CLUSTER_NAME}" \
    --nodegroup-name "${NODEGROUP}" --query 'nodegroup.launchTemplate.{id:id,version:version}' \
    --output text | tr -s "\t" ",")

SECURITY_GROUPS=$(aws eks describe-cluster \
    --name "${CLUSTER_NAME}" --query "cluster.resourcesVpcConfig.clusterSecurityGroupId" --output text)

aws ec2 create-tags \
    --tags "Key=karpenter.sh/discovery,Value=${CLUSTER_NAME}" \
    --resources "${SECURITY_GROUPS}"
```

### 4. Install Karpenter

Export the cluster endpoint and apply the Karpenter manifests:

```bash
export CLUSTER_ENDPOINT="$(aws eks describe-cluster --name "${CLUSTER_NAME}" --query "cluster.endpoint" --output text)"
export KARPENTER_IAM_ROLE_ARN="arn:aws:iam::${AWS_ACCOUNT_ID}:role/${CLUSTER_NAME}-karpenter"

echo "${CLUSTER_ENDPOINT} ${KARPENTER_IAM_ROLE_ARN}"
```

Logout of helm registry to perform an unauthenticated pull against the public ECR:

```bash
helm registry logout public.ecr.aws
```

Install Karpenter with Helm:

```bash
helm upgrade --install karpenter oci://public.ecr.aws/karpenter/karpenter --version "${KARPENTER_VERSION}" \
    --namespace "${KARPENTER_NAMESPACE}" --create-namespace \
    --set "settings.clusterName=${CLUSTER_NAME}" \
    --set "settings.interruptionQueue=${CLUSTER_NAME}" \
    --set controller.resources.requests.cpu=1 \
    --set controller.resources.requests.memory=1Gi \
    --set controller.resources.limits.cpu=1 \
    --set controller.resources.limits.memory=1Gi \
    --wait
```

### 5. Create NodePool and EC2NodeClass

Create a default EC2NodeClass:

```yaml
cat <<EOF | envsubst | kubectl apply -f -
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass
metadata:
  name: default
spec:
  amiFamily: AL2
  subnetSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"
  securityGroupSelectorTerms:
  - tags:
      karpenter.sh/discovery: "${CLUSTER_NAME}"
  instanceStorePolicy: RAID0
  role: "KarpenterNodeInstanceProfile-${CLUSTER_NAME}"
EOF
```

Create a NodePool:

```yaml
cat <<EOF | kubectl apply -f -
apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: default
spec:
  template:
    spec:
      nodeClassRef:
        apiVersion: karpenter.k8s.aws/v1beta1
        kind: EC2NodeClass
        name: default
      requirements:
        - key: kubernetes.io/arch
          operator: In
          values: ["amd64"]
        - key: kubernetes.io/os
          operator: In
          values: ["linux"]
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot", "on-demand"]
        - key: karpenter.k8s.aws/instance-category
          operator: In
          values: ["c", "m", "r"]
        - key: karpenter.k8s.aws/instance-generation
          operator: Gt
          values: ["2"]
  disruption:
    consolidationPolicy: WhenUnderutilized
  limits:
    cpu: 1000
EOF
```
</ProviderContent>

## Test Your Installation

Create a test deployment to verify Karpenter is working:

<ProviderContent providers={["azure"]}>
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-test
spec:
  replicas: 5
  selector:
    matchLabels:
      app: nginx-test
  template:
    metadata:
      labels:
        app: nginx-test
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
```

Apply the deployment:

```bash
kubectl apply -f nginx-test.yaml
```

Watch for new nodes to be provisioned:

```bash
kubectl get nodes -w
```
</ProviderContent>

<ProviderContent providers={["aws"]}>
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      terminationGracePeriodSeconds: 0
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.7
          resources:
            requests:
              cpu: 1
```

Apply the deployment and scale it up:

```bash
kubectl apply -f inflate.yaml
kubectl scale deployment inflate --replicas 5
```

Watch for new nodes to be provisioned:

```bash
kubectl logs -f -n "${KARPENTER_NAMESPACE}" -l app.kubernetes.io/name=karpenter -c controller
```
</ProviderContent>

You should see Karpenter automatically provision new nodes to accommodate the pods that can't be scheduled on existing nodes.

## Clean Up

<ProviderContent providers={["azure"]}>
To avoid ongoing charges, delete the resources you created:

```bash
# Delete the deployment
kubectl delete deployment nginx-test

# Delete the cluster (if using NAP)
az aks delete --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME --yes --no-wait

# Delete the resource group
az group delete --name $RESOURCE_GROUP --yes --no-wait
```
</ProviderContent>

<ProviderContent providers={["aws"]}>
To avoid ongoing charges, delete the resources you created:

```bash
# Delete the test deployment
kubectl delete deployment inflate

# Delete the cluster
eksctl delete cluster --name "${CLUSTER_NAME}"

# Delete the CloudFormation stack
aws cloudformation delete-stack --stack-name "Karpenter-${CLUSTER_NAME}"
```
</ProviderContent>

## Next Steps

- Learn about [NodePools](../concepts/nodepools) and <ProviderContent providers={["azure"]}>[AKSNodeClasses](../concepts/nodeclasses)</ProviderContent><ProviderContent providers={["aws"]}>[EC2NodeClasses](../concepts/nodeclasses)</ProviderContent>
- Explore [scheduling](../concepts/scheduling) and [disruption](../concepts/disruption) concepts
- <ProviderContent providers={["azure"]}>Check out [Azure-specific examples](https://github.com/Azure/karpenter-provider-azure/tree/main/examples)</ProviderContent><ProviderContent providers={["aws"]}>Check out [AWS-specific examples](https://github.com/aws/karpenter-provider-aws/tree/main/examples)</ProviderContent>